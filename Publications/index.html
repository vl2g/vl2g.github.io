<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
	<title> VL2G @ IITJ</title>

	<!-- Bootstrap -->
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" \ 
	integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
	<link rel="stylesheet" type="text/css" href="/static/css/vl2g.css">
	<link href="http://fonts.googleapis.com/css?family=Open+Sans:400,700,600" rel="stylesheet" type="text/css">
	<!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
	<!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
		<!--[if lt IE 9]>
			<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
			<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
			<![endif]-->
		</head>

		<script>
			(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
				(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
				m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
			})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

			ga('create', 'UA-85595941-1', 'auto');
			ga('send', 'pageview');

		</script>

		<body>


			<header>
				<nav class="navbar navbar-default mall-navbar">
			<div class="container">
				<div class="navbar-header">
					<button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
						<span class="sr-only">Toggle navigation</span>
						<span class="icon-bar"></span>
						<span class="icon-bar"></span>
						<span class="icon-bar"></span>
					</button>
					<a class="navbar-brand navbar-left" href="/"><span class="nav-name hidden-xs hidden-sm"> <p style="color:white;">Vision, Language and Learning Group (VL2G) @ IITJ</p></span> <h3 class="abbr-navbar-brand hidden-md hidden-lg hidden-xl" style="line-height: 0px;"><p style="color:white;"> VL2G @ IITJ</h3></a>
				</div>
				<div id="navbar" class="navbar-collapse collapse">
					<ul class="nav navbar-nav navbar-right">
						<li><a href="/"><p style="color:white;">HOME</p></a></li>
						<li><a href="/people/"><p style="color:white;">PEOPLE</p></a></li>
						<li><a href="/publications/"><p style="color:white;">PUBLICATIONS</p></a></li>
						<li><a href="/gallery/"><p style="color:white;">GALLERY</p></a></li>
				<!-- <li><a href="/teaching/">teaching</a></li>
				<li><a href="/news/">news</a></li> -->
				<!-- <li><a href="/resources/">resources</a></li>
				<li><a href="/demos/">demos</a></li> -->
				<!-- <li><a href="/gallery/">gallery</a></li> -->
				
			</ul>
		</div><!--/.nav-collapse -->
	</div>
</nav>
</header>





<div class="container content">
	
	      <li> Answer Mining from a Pool of Images: Towards Retrieval-Based Visual Question Answering,
              <span class="blinking">(NEW)</span>  <br />
            Abhirama Subramanyam Penamakuri, Anand Mishra, Manish Gupta, Mithun Das Gupta,<br/>
                <i> <b>IJCAI 2023. </b> <br />
		 [<a target="_blank" rel="nofollow" href="">Paper</a>][<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/retvqa/">Project Page</a>][<a target="_blank" rel="nofollow" href="">Code</a>] 
            </li>
                <br/>
			    
	    <li> Towards Making Flowchart Images Machine Interpretable,
              <span class="blinking">(NEW)</span>  <br />
            Shreya Shukla, Prajwal Gatti, Yogesh Kumar, Vikash Yadav, Anand Mishra,<br/>
                <i> <b>ICDAR 2023. </b> <br />
		 [<a target="_blank" rel="nofollow" href="">Paper</a>][<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/floco/">Project Page</a>][<a target="_blank" rel="nofollow" href="">Code</a>] 
            </li>
                <br/>
				 
				 
	       	 <li> Few-Shot Referring Relationships in Videos,
              <span class="blinking">(NEW)</span>  <br />
            Yogesh Kumar, Anand Mishra,<br/>
                <i> <b>CVPR 2023. </b> <br />
		 [<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/refRelations/docs/paper.pdf">Paper</a>][<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/refRelations/">Project Page</a>][<a target="_blank" rel="nofollow" href="">Code</a>] 
        </li>
                <br/>

       	 <li> Contrastive Multi-View Textual-Visual Encoding:
Towards One Hundred Thousand-Scale One-Shot Logo Identification,
              <span class="blinking">(NEW)</span>  <br />
            Nakul Sharma, Abhirama Subramanyam Penamakuri, Anand Mishra,<br/>
                <i> <b>ICVGIP 2022. </b> <br />
                [<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/logoIdent/index_files/78.pdf">Paper</a>][<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/logoIdent/">Project Page</a>][<a target="_blank" rel="nofollow" href="">Code</a>] 
        </li>
                <br/>	
	
       	 <li> VISTOT: Vision-Augmented Table-to-Text Generation,
              <span class="blinking">(NEW)</span>  <br />
            Prajwal Gatti, Anand Mishra, Manish Gupta, Mithun Das Gupta,<br/>
                <i> <b>EMNLP 2022. </b> <br />
                [<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/vistot/docs/VISTOT-EMNLP2022.pdf">Paper</a>][<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/vistot/">Project Page</a>][<a target="_blank" rel="nofollow" href="">Code</a>] 
        </li>
                <br/>
            
      
          
          
          <li> COFAR: Commonsense and Factual Reasoning in Image Search
              <span class="blinking">(NEW)</span>  <br />
            Prajwal Gatti, Abhirama Subramanyam Penamakuri, Revant Teotia, Anand Mishra, Shubhashis Sengupta, Roshni Ramnani<br/>
                <i> <b>AACL-IJCNLP 2022. </b> <br />
                [<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/cofar/docs/COFAR-AACL2022.pdf">Paper</a>][<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/cofar/">Project Page</a>][<a target="_blank" rel="nofollow" href="https://github.com/vl2g/cofar">Code</a>] 
        </li>
                <br/>
		

	         <li> Few-shot Visual Relationship Co-localization 
                ,<br />
                 Revant Teotia*, Vaibhav Mishra*, Mayank Maheshwari*, Anand Mishra,<br/>
                <i> <b>ICCV 2021. </b> <br />
                [<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/vrc/docs/VRC-ICCV2021.pdf">Paper</a>][<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/vrc/">Project Page</a>][<a target="_blank" rel="nofollow" href="https://github.com/vl2g/VRC.git">Code</a>] 
                  (*: equal contribution)
                  </li>
                <br/>


	        <li> Look, Attend and Ask: Learning to Ask Questions by Reading Text in Images, <br/>
                Soumya Jahagirdar, Shankar Gangisetty, Anand Mishra<br/>
                <i> <b>ICDAR 2021 </b>. <br />
                [<a target="_blank" rel="nofollow" href="./files/">Paper</a>] 
                </li>
	        <br/>

	
	
                <li>
                Sketch-Guided Object Localization in Natural Images,</span><br />
                Aditay Tripathi, Rajath R. Dani, Anand Mishra, Anirban Chakraborty<br/>
                <i> <b>ECCV 2020 (Spotlight Presentation)</b>. <br />
                [<a target="_blank" rel="nofollow" href="./files/TripathiECCV2020.pdf">Paper</a>] [<a target="_blank" rel="nofollow" href='./files/ECCV2020.bib'>bibtex</a>]
                [<a href="http://visual-computing.in/sketch-guided-object-localization/">Project page</a>][<a href="https://github.com/IISCAditayTripathi/SketchGuidedLocalization">Code</a>]
                [<a href="https://www.youtube.com/watch?v=vI0NEVytLfU">Know the paper in 90 seconds</a>] [<a href="https://www.youtube.com/watch?v=5gBDPbbvssk">Know the paper in ten minutes</a>]
                </li>
                <br/>

                <li>
                From Strings to Things: Knowledge-enabled VQA model that can read and reason,</span><br />
                Ajeet Kumar Singh, Anand Mishra, Shashank Shekhar, and Anirban Chakraborty<br/>
                <i> <b>ICCV 2019 (oral)</b>. <br />
                [<a target="_blank" rel="nofollow" href="https://textkvqa.github.io/docs/textKVQA_ICCV2019.pdf">Paper</a>] [<a target="_blank" rel="nofollow" href='./files/ICCV2019.bib'>bibtex</a>]
                [<a href="https://textkvqa.github.io">Project page</a>]
                </li>
        


</div>


<footer class="widewrapper footer">
	<div class="container">
	  <div class="row footer">
		<div class="col-sm-4">
		  <p ><b><p style="color:white;"> Contact: </b> <br> 
			<p style="color:white;"> Department of Computer Science and Engineering <br>
			Indian Institute of Technology, Jodhpur <br>
			Jodhpur - 342037 (RJ), India <br>
			   
		</div>
		<div class="col-sm-offset-11">
		  <a href="http://www.iitj.ac.in"> <img src="/iitjlogo.jpg" style="width:80px;height:80px;"></a> 
		</div>
	  </div>
	</div>
</footer>

<!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" \ 
integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
</body>
</html>

                
