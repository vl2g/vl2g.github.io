<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
  <title>VL2G @ IITJ - Publications</title>

  <!-- Bootstrap 5 -->
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-T3c6CoIi6uLrA9TneNEoa7RxnatzjcDSCmG1MXxSR1GAsXEV/Dwwykc2MPK8M2HN" crossorigin="anonymous">
  <link rel="stylesheet" type="text/css" href="/static/css/vl2g.css">
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
  
  <!-- Google Analytics -->
  <script>
    (function (i, s, o, g, r, a, m) {
      i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
        (i[r].q = i[r].q || []).push(arguments)
      }, i[r].l = 1 * new Date(); a = s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-85595941-1', 'auto');
    ga('send', 'pageview');
  </script>
</head>

<body>
  <!-- Navigation -->
  <header>
    <nav class="navbar navbar-expand-lg navbar-dark fixed-top">
      <div class="container">
        <a class="navbar-brand fw-semibold" href="/">
          <span class="d-none d-md-inline">Vision, Language, and Learning Group (VL2G) @ IITJ</span>
          <span class="d-md-none">VL2G @ IITJ</span>
        </a>
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarNav">
          <ul class="navbar-nav ms-auto">
            <li class="nav-item"><a class="nav-link" href="/">HOME</a></li>
            <li class="nav-item"><a class="nav-link" href="/people/">PEOPLE</a></li>
            <li class="nav-item"><a class="nav-link active" href="/Publications/">PUBLICATIONS</a></li>
            <li class="nav-item"><a class="nav-link" href="/gallery/">GALLERY</a></li>
          </ul>
        </div>
      </div>
    </nav>
  </header>


<main class="main-content">
  <div class="container py-5">
    <!-- Publications grouped by year using details/summary -->
    <div class="publications-group">
      <details open>
        <summary class="year-summary">2026</summary>
        <ul class="publications-list">
          <li>PatientVLM Meets DocVLM: Pre-Consultation Dialogue Between Vision-Language Models for Efficient Diagnosis<br />
            K Lokesh, Uday Agarwal, Abhirama Subramanyam Penamakuri, Apoorva Challa, Shreya K Gowda, Somesh Gupta, Anand Mishra<br />
            <i><b>AAAI 2026.</b> <span class="badge-new">(NEW)</span></i><br />
            [<a target="_blank" rel="nofollow" href="#">Paper</a>]
            [<a target="_blank" rel="nofollow" href="#">Code</a>] (Coming Soon)
          </li>
          <li>Temporal Object-Aware Vision Transformer for Few-Shot Video Object Detection<br />
            Yogesh Kumar, Anand Mishra<br />
            <i><b>AAAI 2026..</b> <span class="badge-new">(NEW)</span></i><br />
            [<a target="_blank" rel="nofollow" href="#">Paper</a>]
            [<a target="_blank" rel="nofollow" href="#">Code</a>](Coming Soon)
          </li>
                  </ul>
      </details>
      <details open>
        <summary class="year-summary">2025</summary>
        <ul class="publications-list">
          <li>When Big Models Train Small Ones: Label-Free Model Parity Alignment for Efficient Visual Question Answering using Small VLMs<br />
            Abhirama Subramanyam Penamakuri*, Navlika Singh*, Piyush Arora*, Anand Mishra. (*: equal contribution)<br />
            <i><b>EMNLP 2025.</b> <span class="badge-new">(NEW)</span></i><br />
            [<a target="_blank" rel="nofollow" href="https://arxiv.org/abs/2509.16633">Paper</a>]
            [<a target="_blank" rel="nofollow" href="https://github.com/vl2g/MPA">Code</a>]
          </li>
          <li>Aligning Moments in Time using Video Queries<br />
            Yogesh Kumar*, Uday Agarwal*, Manish Gupta, Anand Mishra. (*: equal contribution)<br />
            <i><b>ICCV 2025.</b> <span class="badge-new">(NEW)</span></i><br />
            [<a target="_blank" rel="nofollow" href="https://arxiv.org/pdf/2508.15439">Paper</a>]
            [<a target="_blank" rel="nofollow" href="https://github.com/vl2g/MATR">Code</a>]
          </li>
          <li>SynSlideGen : AI-Generated Lecture Slides for Improving Slide Element Detection and Retrieval<br />
            Suyash Maniyar, Vishvesh Trivedi, Ajoy Mondal, Anand Mishra, C.V. Jawahar<br />
            <i><b>ICDAR 2025 (Oral).</b> <span class="badge-new">(NEW)</span></i><br />
            [<a target="_blank" rel="nofollow" href="https://www.arxiv.org/pdf/2506.23605">Paper</a>]
            [<a target="_blank" rel="nofollow" href="https://synslidegen.github.io/">Project Page</a>]
          </li>
          <li>Audiopedia: Audio QA with Knowledge,<br />
            Abhirama Subramanyam Penamakuri*, Kiran Chhatre*, Akshat Jain.<br />
            <i><b>ICASSP 2025.</b> <span class="badge-new">(NEW)</span></i><br />
            [<a target="_blank" rel="nofollow" href="https://ieeexplore.ieee.org/abstract/document/10889814">Paper</a>]
            [<a target="_blank" rel="nofollow" href="https://abhiram4572.github.io/projects/audiopedia/">Project Page</a>]
            [<a target="_blank" rel="nofollow" href="https://github.com/Abhiram4572/Audiopedia">Data</a>]
          </li>
          <li>PatentLMM: Large Multimodal Model for Generating Descriptions for Patent Figures,<br />
            Shreya Shukla*, Nakul Sharma*, Manish Gupta, Anand Mishra.<br />
            <i><b>AAAI 2025.</b> <span class="badge-new">(NEW)</span></i><br />
            [<a target="_blank" rel="nofollow" href="https://anandmishra22.github.io/files/Shreya-AAAI25.pdf">Paper</a>]
            [<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/PatentLMM/">Project Page</a>]
          </li>
        </ul>
      </details>
      <details open>
        <summary class="year-summary">2024</summary>
        <ul class="publications-list">
          <li>Chapter-Based Video Moment Retrieval using Natural Language Queries,<br />
            Uday Agarwal*, Yogesh Kumar*, Abu Shahid*, Prajwal Gatti, Manish Gupta, Anand Mishra.<br />
            <i><b>ICVGIP 2024.</b></i><br />
            [<a target="_blank" rel="nofollow" href="https://drive.google.com/file/d/143odnhV4CljwX8Y8hZAPThkLbEVJtEdv/view?usp=sharing">Paper</a>]
            [<a target="_blank" rel="nofollow" href="https://github.com/vl2g/ChapVidMR">Code</a>]
            [<a target="_blank" rel="nofollow" href="https://github.com/vl2g/ChapVidMR/tree/main/data">Data</a>]
          </li>
          <li>Visual Text Matters: Improving Text-KVQA with Visual Text Entity Knowledge-aware Large Multimodal Assistant,<br />
            Abhirama Subramanyam Penamakuri, Anand Mishra.<br />
            <i><b>EMNLP 2024.</b></i><br />
            [<a target="_blank" rel="nofollow" href="https://anandmishra22.github.io/files/Abhirama-EMNLP24.pdf">Paper</a>]
            [<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/LMM4Text-KVQA/">Project Page</a>]
          </li>
          <li>Show Me the World in My Language: Establishing the First Baseline for Scene-Text to Scene-Text Translation,<br />
            Shreyas Vaidya*, Arvind Kumar Sharma*, Prajwal Gatti, Anand Mishra. (*: equal contribution)<br />
            <i><b>ICPR 2024.</b></i><br />
            [<a target="_blank" rel="nofollow" href="https://arxiv.org/pdf/2308.03024">Paper</a>]
            [<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/visTrans/">Project Page</a>]
            [<a target="_blank" rel="nofollow" href="https://github.com/Bhashini-IITJ/visualTranslation/">Code</a>]
          </li>
          <li>Sketch-guided Image Inpainting with Partial Discrete Diffusion Process,<br />
            Nakul Sharma, Aditay Tripathi, Anirban Chakraborty, Anand Mishra<br />
            <i><b>CVPR Workshop 2024.</b></i><br />
            [<a target="_blank" rel="nofollow" href="https://arxiv.org/pdf/2404.11949.pdf">Paper</a>]
            [<a target="_blank" rel="nofollow" href="https://github.com/vl2g/Sketch-Inpainting.git">Code</a>]
          </li>
          <li>QDETRv: Query-Guided DETR for One-Shot Object Localization in Videos,<br />
            Yogesh Kumar, Saswat Mallick, Anand Mishra, Sowmya Rasipuram, Anutosh Maitra, Roshni Ramnani<br />
            <i><b>AAAI 2024.</b></i><br />
            [<a target="_blank" rel="nofollow" href="./files/Kumar-AAAI24.pdf">Paper</a>]
            [<a target="_blank" rel="nofollow" href="https://github.com/yogesh-iitj/QDETRV">Code</a>]
          </li>
          <li>Composite Sketch+Text Queries for Retrieving Objects with Elusive Names and Complex Interactions,<br />
            Prajwal Gatti, Kshitij Parikh, Dhriti Paul, Manish Gupta, Anand Mishra.<br />
            <i><b>AAAI 2024.</b></i><br />
            [<a target="_blank" rel="nofollow" href="./files/Gatti-AAAI24.pdf">Paper</a>]
            [<a target="_blank" rel="nofollow" href="https://github.com/vl2g/CSTBIR">Code</a>]
          </li>
        </ul>
      </details>
      <details open>
        <summary class="year-summary">2023</summary>
        <ul class="publications-list">
          <li>Answer Mining from a Pool of Images: Towards Retrieval-Based Visual Question Answering,<br />
            Abhirama Subramanyam Penamakuri, Anand Mishra, Manish Gupta, Mithun Das Gupta,<br />
            <i><b>IJCAI 2023.</b></i><br />
            [<a target="_blank" rel="nofollow" href="https://drive.google.com/file/d/1yANgw3vPpnwRiGKMPjIWWt_kzEMEPjb_/view?pli=1">Paper</a>]
            [<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/retvqa/">Project Page</a>]
            [<a target="_blank" rel="nofollow" href="https://github.com/Abhiram4572/mi_bart">Code</a>]
          </li>
          <li>Towards Making Flowchart Images Machine Interpretable,<br />
            Shreya Shukla, Prajwal Gatti, Yogesh Kumar, Vikash Yadav, Anand Mishra,<br />
            <i><b>ICDAR 2023.</b></i><br />
            [<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/floco/docs/FLOCO-ICDAR2023.pdf">Paper</a>]
            [<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/floco/">Project Page</a>]
            [<a target="_blank" rel="nofollow" href="https://github.com/vl2g/floco">Code</a>]
          </li>
          <li>Few-Shot Referring Relationships in Videos,<br />
            Yogesh Kumar, Anand Mishra,<br />
            <i><b>CVPR 2023.</b></i><br />
            [<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/refRelations/docs/paper.pdf">Paper</a>]
            [<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/refRelations/">Project Page</a>]
            [<a target="_blank" rel="nofollow" href="https://github.com/vl2g/RefRelations">Code</a>]
          </li>
        </ul>
      </details>
      <details open>
        <summary class="year-summary">2022</summary>
        <ul class="publications-list">
          <li>Contrastive Multi-View Textual-Visual Encoding: Towards One Hundred Thousand-Scale One-Shot Logo Identification,<br />
            Nakul Sharma, Abhirama Subramanyam Penamakuri, Anand Mishra,<br />
            <i><b>ICVGIP 2022.</b></i><br />
            [<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/logoIdent/index_files/78.pdf">Paper</a>]
            [<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/logoIdent/">Project Page</a>]
            [<a target="_blank" rel="nofollow" href="https://github.com/thisis-nkul/one-shot-logo_icvgip">Code</a>]
          </li>
          <li>VISTOT: Vision-Augmented Table-to-Text Generation,<br />
            Prajwal Gatti, Anand Mishra, Manish Gupta, Mithun Das Gupta,<br />
            <i><b>EMNLP 2022.</b></i><br />
            [<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/vistot/docs/VISTOT-EMNLP2022.pdf">Paper</a>]
            [<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/vistot/">Project Page</a>]
            [<a target="_blank" rel="nofollow" href="https://github.com/vl2g/visToT">Code</a>]
          </li>
          <li>COFAR: Commonsense and Factual Reasoning in Image Search<br />
            Prajwal Gatti, Abhirama Subramanyam Penamakuri, Revant Teotia, Anand Mishra, Shubhashis Sengupta, Roshni Ramnani<br />
            <i><b>AACL-IJCNLP 2022.</b></i><br />
            [<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/cofar/docs/COFAR-AACL2022.pdf">Paper</a>]
            [<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/cofar/">Project Page</a>]
            [<a target="_blank" rel="nofollow" href="https://github.com/vl2g/cofar">Code</a>]
          </li>
        </ul>
      </details>
      <details open>
        <summary class="year-summary">2021</summary>
        <ul class="publications-list">
          <li>Few-shot Visual Relationship Co-localization,<br />
            Revant Teotia*, Vaibhav Mishra*, Mayank Maheshwari*, Anand Mishra,<br />
            <i><b>ICCV 2021.</b></i><br />
            [<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/vrc/docs/VRC-ICCV2021.pdf">Paper</a>]
            [<a target="_blank" rel="nofollow" href="https://vl2g.github.io/projects/vrc/">Project Page</a>]
            [<a target="_blank" rel="nofollow" href="https://github.com/vl2g/VRC">Code</a>]
            (*: equal contribution)
          </li>
          <li>Look, Attend and Ask: Learning to Ask Questions by Reading Text in Images,<br />
            Soumya Jahagirdar, Shankar Gangisetty, Anand Mishra<br />
            <i><b>ICDAR 2021.</b></i><br />
            [<a target="_blank" rel="nofollow" href="./files/">Paper</a>]
          </li>
        </ul>
      </details>
      <details open>
        <summary class="year-summary">2020</summary>
        <ul class="publications-list">
          <li>Sketch-Guided Object Localization in Natural Images,<br />
            Aditay Tripathi, Rajath R. Dani, Anand Mishra, Anirban Chakraborty<br />
            <i><b>ECCV 2020 (Spotlight Presentation).</b></i><br />
            [<a target="_blank" rel="nofollow" href="./files/TripathiECCV2020.pdf">Paper</a>]
            [<a target="_blank" rel="nofollow" href='./files/ECCV2020.bib'>bibtex</a>]
            [<a href="http://visual-computing.in/sketch-guided-object-localization/">Project page</a>]
            [<a href="https://github.com/IISCAditayTripathi/SketchGuidedLocalization">Code</a>]
            [<a href="https://www.youtube.com/watch?v=vI0NEVytLfU">Know the paper in 90 seconds</a>]
            [<a href="https://www.youtube.com/watch?v=5gBDPbbvssk">Know the paper in ten minutes</a>]
          </li>
        </ul>
      </details>
      <details open>
        <summary class="year-summary">2019</summary>
        <ul class="publications-list">
          <li>From Strings to Things: Knowledge-enabled VQA model that can read and reason,<br />
            Ajeet Kumar Singh, Anand Mishra, Shashank Shekhar, and Anirban Chakraborty<br />
            <i><b>ICCV 2019 (oral).</b></i><br />
            [<a target="_blank" rel="nofollow" href="https://textkvqa.github.io/docs/textKVQA_ICCV2019.pdf">Paper</a>]
            [<a target="_blank" rel="nofollow" href='./files/ICCV2019.bib'>bibtex</a>]
            [<a href="https://textkvqa.github.io">Project page</a>]
          </li>
        </ul>
      </details>
    </div>
</main>

<!-- Footer -->
  <footer class="footer mt-5 py-4 bg-light">
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
          <a href="http://www.iitj.ac.in" target="_blank" rel="noopener">
            <img src="/iitjlogo-with-name.png" alt="IIT Jodhpur" style="height:100px" class="img-fluid">
          </a>
        </div>
      </div>
    </div>
  </footer>

  <!-- Bootstrap 5 JS Bundle -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-C6RzsynM9kWDrMNeT87bh95OGNyZPhcTNXj1NW7RuBCsyN/o0jlpcV8Qyq46cDfL" crossorigin="anonymous"></script>
</body>

</html>